{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet10 with CIFAR10 dataset - Study Optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "732RXgD-pCXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8bSmKbGpBfz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from torchsummary import summary\n",
        "except ModuleNotFoundError:\n",
        "    !pip install torchsummary\n",
        "    from torchsummary import summary\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.RandomAffine(degrees=10, shear = 10),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "train = datasets.CIFAR10(root = './data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "\n",
        "# Do we have CUDA drivers for us?\n",
        "cuda = torch.cuda.is_available()\n",
        "print (\"Cuda Available?\", cuda)\n",
        "\n",
        "dataloader_args = dict(shuffle=True, batch_size=2048, num_workers=2, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, **dataloader_args)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, **dataloader_args)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_yxR2s92RqM",
        "outputId": "9036df8c-75b1-48ac-f4f6-1c84d73346bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:19<00:00, 8.95MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Cuda Available? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv01 = nn.Conv2d(3, 16, 3, bias=False, padding=1)\n",
        "        self.batch01 = nn.BatchNorm2d(num_features=16)\n",
        "\n",
        "        # ---- Lets take a skip connection\n",
        "        self.skip_conv1 = nn.Conv2d(16, 16, 3, padding=0, dilation=2)\n",
        "\n",
        "        self.conv02 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n",
        "        self.batch02 = nn.BatchNorm2d(num_features=16)\n",
        "        self.conv03 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n",
        "        self.batch03 = nn.BatchNorm2d(num_features=16)\n",
        "        self.conv04 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n",
        "        self.batch04 = nn.BatchNorm2d(num_features=16)\n",
        "        self.pool01 = nn.MaxPool2d(2, 2)                                #O=16\n",
        "        self.conv05 = nn.Conv2d(16, 16, 1, bias=False)\n",
        "\n",
        "        self.conv11 = nn.Conv2d(16, 32, 3, bias=False, padding=1)\n",
        "        self.batch11 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv12 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n",
        "        self.batch12 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv13 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n",
        "        self.batch13 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv14 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n",
        "        self.batch14 = nn.BatchNorm2d(num_features=32)\n",
        "        self.pool11 = nn.MaxPool2d(2, 2)                                #O=8\n",
        "        self.conv15 = nn.Conv2d(32, 32, 1, bias=False)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n",
        "        self.batch21 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv22 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n",
        "        self.batch22 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv23 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n",
        "        self.batch23 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv24 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n",
        "        self.batch24 = nn.BatchNorm2d(num_features=64)\n",
        "        self.pool21 = nn.MaxPool2d(2, 2)                                #O=4\n",
        "        self.conv25 = nn.Conv2d(64, 64, 1, bias=False)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, groups=64, bias = False, padding = 1)\n",
        "        self.convPV1= nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, bias = False, padding = 0)\n",
        "        self.batch31 = nn.BatchNorm2d(num_features=128)\n",
        "        self.conv32 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n",
        "        self.convPV2= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, bias = False, padding = 0)\n",
        "        self.batch32 = nn.BatchNorm2d(num_features=256)\n",
        "\n",
        "\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n",
        "        self.convx3 = nn.Conv2d(256, 10, 1, bias=False, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batch01(F.relu(self.conv01(x)))\n",
        "\n",
        "        # ---- Lets take a skip connection\n",
        "        skip_channels = self.skip_conv1(self.skip_conv1(self.skip_conv1(self.skip_conv1(x))))\n",
        "\n",
        "        x = self.batch02(F.relu(self.conv02(x)))\n",
        "        x = self.batch03(F.relu(self.conv03(x)))\n",
        "        x = self.batch04(F.relu(self.conv04(x)))\n",
        "        x = self.pool01(x)\n",
        "        x = self.conv05(x)\n",
        "        # ----------------------------------------------------------\n",
        "\n",
        "        # ---- Lets add the skip connection here\n",
        "        x = skip_channels + x\n",
        "\n",
        "        x = self.batch11(F.relu(self.conv11(x)))\n",
        "        x = self.batch12(F.relu(self.conv12(x)))\n",
        "        x = self.batch13(F.relu(self.conv13(x)))\n",
        "        x = self.batch14(F.relu(self.conv14(x)))\n",
        "        x = self.pool11(x)\n",
        "        x = self.conv15(x)\n",
        "        # ----------------------------------------------------------\n",
        "\n",
        "        x = self.batch21(F.relu(self.conv21(x)))\n",
        "        x = self.batch22(F.relu(self.conv22(x)))\n",
        "        x = self.batch23(F.relu(self.conv23(x)))\n",
        "        x = self.batch24(F.relu(self.conv24(x)))\n",
        "        x = self.pool21(x)\n",
        "        x = self.conv25(x)\n",
        "        # ----------------------------------------------------------\n",
        "\n",
        "        x = self.batch31(F.relu(self.convPV1(F.relu(self.conv31(x)))))\n",
        "        x = self.batch32(F.relu(self.convPV2(F.relu(self.conv32(x)))))\n",
        "\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.convx3(x)\n",
        "        x = x.view(-1, 10)                           # Don't want 10x1x1..\n",
        "        return F.log_softmax(x, dim=1)  # Added dim=1 parameter)"
      ],
      "metadata": {
        "id": "K10_ILasJFly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZsEKXnM7Iao",
        "outputId": "2e87abea-0633-4381-da9c-c767d3f8190f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 28, 28]           2,320\n",
            "            Conv2d-4           [-1, 16, 24, 24]           2,320\n",
            "            Conv2d-5           [-1, 16, 20, 20]           2,320\n",
            "            Conv2d-6           [-1, 16, 16, 16]           2,320\n",
            "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "           Conv2d-11           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
            "        MaxPool2d-13           [-1, 16, 16, 16]               0\n",
            "           Conv2d-14           [-1, 16, 16, 16]             256\n",
            "           Conv2d-15           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-16           [-1, 32, 16, 16]              64\n",
            "           Conv2d-17           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-18           [-1, 32, 16, 16]              64\n",
            "           Conv2d-19           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-20           [-1, 32, 16, 16]              64\n",
            "           Conv2d-21           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-22           [-1, 32, 16, 16]              64\n",
            "        MaxPool2d-23             [-1, 32, 8, 8]               0\n",
            "           Conv2d-24             [-1, 32, 8, 8]           1,024\n",
            "           Conv2d-25             [-1, 64, 8, 8]          18,432\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "           Conv2d-27             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
            "           Conv2d-29             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-30             [-1, 64, 8, 8]             128\n",
            "           Conv2d-31             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-32             [-1, 64, 8, 8]             128\n",
            "        MaxPool2d-33             [-1, 64, 4, 4]               0\n",
            "           Conv2d-34             [-1, 64, 4, 4]           4,096\n",
            "           Conv2d-35             [-1, 64, 4, 4]             576\n",
            "           Conv2d-36            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-37            [-1, 128, 4, 4]             256\n",
            "           Conv2d-38            [-1, 128, 4, 4]           1,152\n",
            "           Conv2d-39            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-40            [-1, 256, 4, 4]             512\n",
            "        AvgPool2d-41            [-1, 256, 1, 1]               0\n",
            "           Conv2d-42             [-1, 10, 1, 1]           2,560\n",
            "================================================================\n",
            "Total params: 230,192\n",
            "Trainable params: 230,192\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.22\n",
            "Params size (MB): 0.88\n",
            "Estimated Total Size (MB): 3.11\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "time_taken = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    time_taken.clear()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate (pbar):\n",
        "        t0 = time.time()\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        #Don't want history of gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_predict = model(data)\n",
        "\n",
        "        # Caluclate loss\n",
        "        loss = F.nll_loss(y_predict, target)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        # Back propogate error\n",
        "        loss.backward()\n",
        "\n",
        "        # Take a optimzer step\n",
        "        optimizer.step()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "\n",
        "        time_taken.append((t1-t0))\n",
        "\n",
        "        pred = y_predict.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        # print(f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f} Time taken per iter = {dt :.2f}ms')\n",
        "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)\n",
        "\n",
        "\n",
        "def test (model, device, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "\n",
        "model =  Net().to(device)\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    print(f\" --> EPOCH: {epoch}, Avg Time Taken = {(sum(time_taken)/len(time_taken))*1000:.2f}ms\")\n",
        "    # scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "MMVJZShF7YfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617aeb07-7498-45c9-81ce-38f44622e5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.8593858480453491 Batch_id=24 Accuracy=19.23: 100%|██████████| 25/25 [00:16<00:00,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 0, Avg Time Taken = 328.27ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3060, Accuracy: 1220/10000 (12.20%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.6051859855651855 Batch_id=24 Accuracy=36.13: 100%|██████████| 25/25 [00:15<00:00,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 1, Avg Time Taken = 313.14ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6595, Accuracy: 4036/10000 (40.36%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.412627935409546 Batch_id=24 Accuracy=45.46: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 2, Avg Time Taken = 315.35ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4533, Accuracy: 4719/10000 (47.19%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.275116205215454 Batch_id=24 Accuracy=52.21: 100%|██████████| 25/25 [00:16<00:00,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 3, Avg Time Taken = 321.44ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2846, Accuracy: 5325/10000 (53.25%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.163625955581665 Batch_id=24 Accuracy=55.95: 100%|██████████| 25/25 [00:16<00:00,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 4, Avg Time Taken = 324.29ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1981, Accuracy: 5661/10000 (56.61%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if BF16 is available or not. If not available, progress with FP16"
      ],
      "metadata": {
        "id": "AqznsN0D9hjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def check_bf16_support():\n",
        "    if torch.cuda.is_available():\n",
        "        compute_capability = torch.cuda.get_device_capability()\n",
        "        # Ampere (8.x) and newer GPUs support BF16\n",
        "        return compute_capability[0] >= 8\n",
        "    return False\n",
        "\n",
        "check_bf16_support()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSw1HmpymZoZ",
        "outputId": "8d608638-9b9b-4ba6-e895-1d9b3ae44c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization 1"
      ],
      "metadata": {
        "id": "M8DpOUB7PQRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler  # Updated import\n",
        "torch.backends.cudnn.benchmark = True\n",
        "n_epochs = 5"
      ],
      "metadata": {
        "id": "Ksr9AaehNmrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler  # Updated import\n",
        "torch.backends.cudnn.benchmark = True\n",
        "n_epochs = 5\n",
        "\n",
        "# 1. Setup model and optimizer with FP16 support\n",
        "def setup_fp16_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.compile(Net().to(device))\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=n_epochs)\n",
        "    # Updated GradScaler initialization\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    return model, optimizer, scaler, device\n",
        "\n",
        "# 2. Modified training loop\n",
        "def train(model, device, train_loader, optimizer, scaler, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    time_taken.clear()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Convert data to FP16 before moving to GPU\n",
        "        data = data.half()  # Convert to FP16\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Updated autocast\n",
        "        with autocast('cuda', dtype=torch.float16):\n",
        "            y_predict = model(data)\n",
        "            loss = F.nll_loss(y_predict, target)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # scheduler.step()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "\n",
        "        time_taken.append((t1-t0))\n",
        "\n",
        "        pred = y_predict.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Loss={loss.item()} Batch_idx={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)\n",
        "\n",
        "# 3. Modified test loop\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Convert data to FP16 before moving to GPU\n",
        "            data = data.half()  # Convert to FP16\n",
        "\n",
        "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "\n",
        "            # Updated autocast\n",
        "            with autocast('cuda'):\n",
        "                output = model(data)\n",
        "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "# 4. Training setup and execution\n",
        "model, optimizer, scaler, device = setup_fp16_model()\n",
        "\n",
        "# Your training loop\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, scaler, epoch)\n",
        "    print(f\" --> EPOCH: {epoch}, Avg Time Taken = {(sum(time_taken)/len(time_taken))*1000:.2f}ms\")\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TVLe-WnocLm",
        "outputId": "d8d5e4fa-562f-486d-8a9a-4e01cd21fe00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.7873820066452026 Batch_idx=24 Accuracy=22.39: 100%|██████████| 25/25 [01:51<00:00,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 1, Avg Time Taken = 3980.95ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2954, Accuracy: 1000/10000 (10.00%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.529903769493103 Batch_idx=24 Accuracy=38.04: 100%|██████████| 25/25 [00:16<00:00,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 2, Avg Time Taken = 193.69ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6572, Accuracy: 3924/10000 (39.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.4604082107543945 Batch_idx=24 Accuracy=45.82: 100%|██████████| 25/25 [00:15<00:00,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 3, Avg Time Taken = 132.86ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4653, Accuracy: 4748/10000 (47.48%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.3425822257995605 Batch_idx=24 Accuracy=51.11: 100%|██████████| 25/25 [00:14<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 4, Avg Time Taken = 129.35ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.3570, Accuracy: 5103/10000 (51.03%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2566548585891724 Batch_idx=24 Accuracy=55.15: 100%|██████████| 25/25 [00:14<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 5, Avg Time Taken = 128.22ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1881, Accuracy: 5758/10000 (57.58%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRmfbiA6-GwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization 2"
      ],
      "metadata": {
        "id": "SatKUwwSPVTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler  # Updated import\n",
        "torch.backends.cudnn.benchmark = True\n",
        "n_epochs = 5\n",
        "\n",
        "# 1. Setup model and optimizer with FP16 support\n",
        "def setup_fp16_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.compile(Net().to(device))\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=n_epochs)\n",
        "    # Updated GradScaler initialization\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    return model, optimizer, scaler, device"
      ],
      "metadata": {
        "id": "a_73OubeIEAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9LC1BA4CZmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 512  # Reduced from 2048 for better GPU utilization\n",
        "        self.num_workers = 4\n",
        "        self.base_lr = 0.1\n",
        "        self.weight_decay = 1e-4\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Optimized data loading\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # Better augmentation\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Optimized dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms),\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./data', train=False, transform=test_transforms),\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "def train(model, device, train_loader, optimizer, scaler, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    time_taken.clear()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Efficient data transfer\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "\n",
        "        # Memory efficient gradient zeroing\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast('cuda', dtype=torch.float16):\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Scale loss and backprop\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Timing\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "        time_taken.append((t1-t0))\n",
        "\n",
        "        # Accuracy calculation\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(\n",
        "            desc=f'Loss={loss.item():.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}'\n",
        "        )\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad(), autocast('cuda', dtype=torch.float16):\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\\n')\n",
        "\n",
        "# Setup training\n",
        "torch.backends.cudnn.benchmark = True\n",
        "model = Net().to(device)\n",
        "model = torch.compile(model, mode='reduce-overhead')\n",
        "\n",
        "# Optimized learning rate and optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config.base_lr,\n",
        "    weight_decay=config.weight_decay,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=config.base_lr,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.2,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, scaler, epoch)\n",
        "    avg_time = sum(time_taken)/len(time_taken)\n",
        "    print(f\" --> EPOCH: {epoch}, Avg Time Taken = {avg_time*1000:.2f}ms\")\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btcw4ae3LKoQ",
        "outputId": "7f4650b3-3e16-4f43-f466-e7430d68f023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Loss=1.5346 Batch_id=97 Accuracy=37.89: 100%|██████████| 98/98 [00:50<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 0, Avg Time Taken = 388.70ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4347, Accuracy: 4839/10000 (48.39%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/98 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Loss=1.1719 Batch_id=97 Accuracy=53.36: 100%|██████████| 98/98 [00:17<00:00,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 1, Avg Time Taken = 52.39ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.3349, Accuracy: 5581/10000 (55.81%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/98 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Loss=1.1251 Batch_id=97 Accuracy=60.46: 100%|██████████| 98/98 [00:15<00:00,  6.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 2, Avg Time Taken = 40.80ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1860, Accuracy: 6048/10000 (60.48%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9436 Batch_id=97 Accuracy=65.88: 100%|██████████| 98/98 [00:18<00:00,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 3, Avg Time Taken = 39.15ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9887, Accuracy: 6731/10000 (67.31%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8622 Batch_id=97 Accuracy=69.71: 100%|██████████| 98/98 [00:16<00:00,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 4, Avg Time Taken = 37.46ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9661, Accuracy: 6743/10000 (67.43%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPEuFVdqLKlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZULxyVGLKjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZElsUy8LKg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NdAbFooLKeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kwqeo1LLKbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code for BFP16 support"
      ],
      "metadata": {
        "id": "fEAgBmMo-aHl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xPDxUfM-GqO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}